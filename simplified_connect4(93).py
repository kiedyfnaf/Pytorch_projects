# -*- coding: utf-8 -*-
"""Connect4(93%"max").ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RiQq5aR9ogLVFf_agUUggeAkCWcEQzxx
"""

import torch
import numpy as np
import os
import random
import math
import time
import matplotlib.pyplot as plt
from torch import nn
import torch.optim as optim
import torch.nn.functional as F

device = "cuda" if torch.cuda.is_available() else "cpu"
PERCENT = 95
GAMES = 1000

class Connect4Solver(nn.Module):
    def __init__(self):
        super(Connect4Solver, self).__init__()
        self.layer1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)
        self.activation1 = nn.Tanh()
        self.layer2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)
        self.activation2 = nn.ReLU()
        self.layer3 = nn.Linear(64*6*7, 128, bias=True)
        self.activation3 = nn.Tanh()
        self.output = nn.Linear(128, 7, bias=True)  # Output size is 81 * 9 (possible values for each cell)
        self.softmax = nn.Softmax(dim=-1)

    def forward(self, x):
        x = self.activation1(self.layer1(x))
        x = self.activation2(self.layer2(x))
        x = x.view(64 * 7 * 6)
        x = self.activation3(self.layer3(x))  # New layer and activation
        logits = self.output(x)  # Output logits
        probabilities = logits.view(7)  # Reshape to (81, 9)
        probabilities = self.softmax(probabilities)  # Apply softmax to get probabilities
        return probabilities

    def select_most_confident_cell(self, probabilities):
        max_probability = probabilities.max().item()  # Get the maximum probability
        most_confident_cell = probabilities.argmax().item()  # Get the index of the most confident cell
        return max_probability, most_confident_cell

    def fill_the_number(self, most_confident_cell, board, max_probability):
        stop = False
        while stop == False:
          for index in range(6):
            if board[most_confident_cell][5-index] == 0:
              board[most_confident_cell][5-index] = 1
              return board, most_confident_cell, index
              stop = True
              break
          probabilities[most_confident_cell] = 0
          max_probability, most_confident_cell = self.select_most_confident_cell(probabilities)
          if max_probability == 0:
            return board, most_confident_cell, None
        return board, most_confident_cell, index

    def random_enemy(self, board):
        options = list(range(7))  # List of all possible columns
        random.shuffle(options)  # Shuffle to ensure randomness
        x = "None"
        for col in options:
            if torch.min(board[col]) == 0:
                x = col
                break
        if x == "None":
            return board, "None", "None"
        for i_enemy in range(6):
            if board[x][5 - i_enemy] == 0:
                board[x][5 - i_enemy] = 2
                break
        return board, x, i_enemy

    def in_a_row(self, board, x, most_confident_cell, index):
        count = 0
        if 2 in (board[most_confident_cell][3], board[most_confident_cell][2], board[most_confident_cell][1]):
            count = -1
            return count
        for num in range(4):
            row = most_confident_cell
            col = 5 - index + num
            if col < 0 or col >= len(board[0]):
                return count
            if board[row][col] == 1:  # Player's token
                count += 1
            elif board[row][col] == 2:  # Opponent's token
                return count  # End streak if opponent token is found
        return count

    def block_in_a_row(self, board, x, most_confident_cell, index):
        count_block = 0
        for num in range(4):
            row = most_confident_cell
            col = 6 - index + num
            if col < 1 or col >= len(board[0]):
                return count_block
            if board[row][col] == 2:  # Player's token
                count_block += 1
            elif board[row][col] == 1:  # Opponent's token
                return count_block  # End streak if opponent token is found
        return count_block

    def reward(self, in_a_row, block_in_a_row, result):
        reward = 0
        if in_a_row == -1:
            reward -= 10
        if in_a_row == 1:
            reward += 4
        elif in_a_row == 2:
          reward += 9
        elif in_a_row == 3:
          reward += 15
        if block_in_a_row == 1:
            reward += 1
        elif block_in_a_row == 2:
          reward += 8
        elif block_in_a_row == 3:
          reward += 15
        if result == "Pytorch":
            reward += 100
        elif result == "Drunken":
            reward -= 100
        elif result == "No winner":
            reward -= 50
        return reward

    def is_it_the_end(self, board, x, most_confident_cell, index, i_enemy):
        count, count_enemy= 0, 0
        for num in range(4):
          try:
            if board[most_confident_cell][5-index+num] == 1:
              count += 1
            if count == 4:
              return "Pytorch"
          except IndexError:
            pass
          try:
            if board[x][5-i_enemy+num] == 2:
              count_enemy += 1
            if count_enemy == 4:
              return "Drunken"
          except IndexError:
            pass
        return "No winner"

    def compute_loss(self, reward, epoch):
        if reward > 0:
            loss = max(0, 1 - (reward / 100))  # Reward above 0 decreases loss
        else:
            loss = 1 + abs(reward) / 100  # Negative rewards increase loss
        #epoch_factor = math.sqrt(epoch) / 10  # Adjust the factor as needed
        #loss *= epoch_factor
        return torch.tensor(loss, dtype=torch.float32, requires_grad=True)

model = Connect4Solver()

optimizer = optim.Adam(model.parameters(), lr=0.01)

average = 0
wins = 0
lost = 0
for _ in range(GAMES):
  board = torch.zeros(7,6, device=device)
  board_train = board.reshape(1, 1, 7, 6)
  for epoch in range(50):
    model.train()
    rotated_board = torch.flip(board.T, dims=[1])
    probabilities = model.forward(board_train)
    max_probability, most_confident_cell = model.select_most_confident_cell(probabilities)
    board, most_confident_cell, index = model.fill_the_number(most_confident_cell, board, max_probability)
    if index == None:
      result = "No winner"
      break
    board, x, i_enemy = model.random_enemy(board)
    if x == "None":
      result = "No winner"
      break
    else:
      result = model.is_it_the_end(board, x, most_confident_cell, index, i_enemy)
    in_a_row = model.in_a_row(board, x, most_confident_cell, index)
    block_in_a_row = model.block_in_a_row(board, x, most_confident_cell, index)
    reward = model.reward(in_a_row, block_in_a_row, result)
    board_train = board.view(1,1,7,6)
    if result != "No winner":
      if result == "Pytorch":
        wins += 1
      else:
        print(board)
        lost += 1
      break
    loss = model.compute_loss(reward, epoch)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    model.eval()
  print(f"Game: {_}, loss: {loss}, epoch: {epoch}")
  average += loss.item()
rotated_board = torch.flip(board.T, dims=[1])
print(result)
print(average/GAMES)
print(f"Wins: {100*wins/GAMES}%")
print(f"Lost: {100*lost/GAMES}%")
print(rotated_board)

_

rotated_board

prob = torch.tensor([5,1,2,3,4,3,2])
most = prob.argmax().item()
print(most)

tensor = torch.tensor([0,0,0,0,2,2])
zeros = tensor[tensor == 0]
count_zeros = zeros.numel()
print("Number:", len(tensor) - count_zeros)









